{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEUoIty4i9TB",
        "outputId": "e8fb0a35-939c-4fe0-9c78-e99c8131f5be"
      },
      "source": [
        "%cd '/content/drive/MyDrive/Experiments/PyTorch/auto-punctuation-pytorch'\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Experiments/PyTorch/auto-punctuation-pytorch\n",
            "/content/drive/MyDrive/Experiments/PyTorch/auto-punctuation-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAXpObew3gV_",
        "outputId": "98dd82c5-4cc4-4aa6-f27a-09e69b440665"
      },
      "source": [
        "# GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# RAM\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Nov 25 14:55:29 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MreEupa7qh_N",
        "outputId": "9b6b12bf-e0b5-4ee0-c175-382bb8608ad8"
      },
      "source": [
        "# This is requred to read the .h5 files.\n",
        "!pip install --upgrade tables"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tables in /usr/local/lib/python3.6/dist-packages (3.6.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from tables) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hjbWGY3rxpo"
      },
      "source": [
        "# ðŸ”¥ PyTorch + Auto punctuation\n",
        "\n",
        "I used PyTorch and Bi-directional LSTMs to train an auto punctuation model.\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "<img src=\"https://i.imgur.com/3qUpwC0.png\" width=\"650\" alt=\"Bi-LSTMs\" />\n",
        "\n",
        "<div><img /></div>\n",
        "\n",
        "\n",
        "## What this code has:\n",
        "\n",
        "I show you how to train and validate a light-weight & simple auto-punctuation model. That includes:\n",
        "\n",
        "1. The parameters are stored in `config`.\n",
        "2. The `TRAINING`  flags decides whether you want to train the model our evaluate based on pretrained weights.\n",
        "3. If `TRAINING=False`, the you should set the `PRETRAINED_EPOCH` to the desired number based on the pretrained model you decide yo load.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "Dataset from: [IWSLT 2012 Evaluation Campaign](http://hltc.cs.ust.hk/iwslt/index.php/evaluation-campaign/ted-task.html\n",
        "\n",
        "According to the above link:\n",
        "\n",
        "> \"The IWSLT 2012 Evaluation Campaign includes the TED Task, that is the      translation of TED Talks, a collection of public speeches on a variety of topics. Three tracks are proposed addressing different research tasks:\n",
        "\n",
        "    ASR track : automatic transcription of talks from audio to text (in English)\n",
        "    SLT track: speech translation of talks from audio (or ASR output) to text (from English to French)\n",
        "    MT track : text translation of talks for two language pairs plus ten optional language pairs.\"\n",
        "\n",
        "\n",
        "I used the dataset associated with <font color=blue> ASR track </font>.\n",
        "\n",
        "* I did some cleaning on the text and stored the cleaned text as a new file.\n",
        "* The new file is named \"data.h5\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCdoYu-SxrMI"
      },
      "source": [
        "## Import Requred Libraries & Setup\n",
        "\n",
        "In order to have a better reproducibility, we use seeds for `random`, `NumPy` and `PyTorch` libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTVcEJRxyKlI"
      },
      "source": [
        "# Pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from termcolor import cprint, colored\n",
        "\n",
        "# Other libraries needed\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW5VQOvCyUx3"
      },
      "source": [
        "### Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLj0gmOLyU7n"
      },
      "source": [
        "# For deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash('setting random seed') % 2 ** 32 - 1)\n",
        "np.random.seed(hash('To further improve reproducibility') % 2 ** 32 - 1)\n",
        "torch.manual_seed(hash('Sets a random seed from pytorch random number generators') % 2 ** 32 - 1)\n",
        "torch.cuda.manual_seed_all(hash('Reproducibility') % 2 ** 32 - 1)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s72tpYWyL_i"
      },
      "source": [
        "## Setup Parameters\n",
        "\n",
        "The below parameters needs to be set:\n",
        "\n",
        "* EPOCH_TO_SHOW_PREDICTION: After how many epochs the evaluation results must be show?\n",
        "* batch_size: The batch size for mini-batch optimization\n",
        "* NUM_EPOCHS: Number of epochs to train the model.\n",
        "* TRAINING: Whether we are going to train the model or test it by pretrained models. Type: **boolean**.\n",
        "* PRETRAINED_EPOCH: Which pretrained model we should load. Type: **int**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZdIrk8zE0m"
      },
      "source": [
        "\"\"\" # Some parameter\n",
        "\"\"\"\n",
        "config = dict(\n",
        "    EPOCH_TO_SHOW_PREDICTION=1,\n",
        "    batch_size=128,\n",
        "    NUM_EPOCHS=30,\n",
        "    TRAINING=False,\n",
        "    PRETRAINED_EPOCH = 30\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ZLc06PzHMQ"
      },
      "source": [
        "## Data\n",
        "\n",
        "Here I load the data and prepare it for training purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNpX2-MMzT0I",
        "outputId": "45c5c815-93ce-4095-f3f6-e75e12925da0"
      },
      "source": [
        "# Data path\n",
        "df_path = os.path.expanduser(\"data/data.h5\")\n",
        "\n",
        "# Load hdf5 data\n",
        "data_df = pd.read_hdf(df_path, 'df')\n",
        "loaded_dataset = list(data_df.text)\n",
        "\n",
        "# All the text\n",
        "text = ' '.join(loaded_dataset)\n",
        "\n",
        "# The unique characters in the file\n",
        "chars = sorted(set(text))\n",
        "print('{} unique characters'.format(len(chars)))\n",
        "\n",
        "# Desired characters as punctuations\n",
        "input_chars = list(\"abcdefghijklmnopqrstuvwxyz01234567890\") + [\" \"]\n",
        "output_chars = [\"<nopunc>\", \"<cap>\", \".\", \",\", \"?\", \"!\"]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "97 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPonuXY9zVdJ"
      },
      "source": [
        "### Helper functions and classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRTDyT9szdXF"
      },
      "source": [
        "class CharMap():\n",
        "    def __init__(self, chars=None, add_unknown=False):\n",
        "        self.chars = chars\n",
        "        self.char2idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx2char = np.array(chars)\n",
        "        self.size = len(self.chars)\n",
        "        self.add_unknown = add_unknown\n",
        "        if add_unknown:\n",
        "            self.char2idx['<unk>'] = self.size\n",
        "            self.size += 1\n",
        "    def get_ind(self, char):\n",
        "        try:\n",
        "            return self.char2idx[char]\n",
        "        except KeyError:\n",
        "            if self.add_unknown is False:\n",
        "                raise KeyError('character is not in dictionary: ' + str([char]))\n",
        "            return self.char2idx['<unk>']\n",
        "\n",
        "    def char_code_batch(self, batch):\n",
        "        return torch.LongTensor([[self.char2idx[char] for char in seq] for seq in batch])\n",
        "\n",
        "    def vec2list_batch(self, vec):\n",
        "        chars = [[self.chars[ind] for ind in row] for row in vec.cpu().data.numpy()]\n",
        "        return chars\n",
        "\n",
        "char2vec = CharMap(chars=input_chars, add_unknown=True)\n",
        "output_char2vec = CharMap(chars=output_chars, add_unknown=False)\n",
        "\n",
        "def add_punctuation(text_input, punctuation):\n",
        "    assert len(text_input) == len(punctuation), \"input string has differnt length from punctuation list\" + \"\".join(\n",
        "        text_input) + str(punctuation) + str(len(text_input)) + \";\" + str(len(punctuation))\n",
        "    result = \"\"\n",
        "    for char1, char2 in zip(text_input, punctuation):\n",
        "        if char2 == \"<cap>\":\n",
        "            result += char1.upper()\n",
        "        elif char2 == \"<nopunc>\":\n",
        "            result += char1\n",
        "        else:\n",
        "            result += char2 + char1\n",
        "    return result\n",
        "\n",
        "def extract_punc(string_input, input_chars, output_chars):\n",
        "    input_source = []\n",
        "    output_source = []\n",
        "    input_length = len(string_input)\n",
        "    i = 0\n",
        "    while i < input_length:\n",
        "        char = string_input[i]\n",
        "        if char.isupper():\n",
        "            output_source.append(\"<cap>\")\n",
        "            input_source.append(char.lower())\n",
        "\n",
        "        if char in output_chars:\n",
        "            output_source.append(char)\n",
        "            if i < input_length - 1:\n",
        "                input_source.append(string_input[i + 1])\n",
        "            else:\n",
        "                input_source.append(\" \")\n",
        "            i += 1\n",
        "\n",
        "        if not char.isupper() and char not in output_chars and char in input_chars:\n",
        "            input_source.append(char)\n",
        "            output_source.append(\"<nopunc>\")\n",
        "\n",
        "        i += 1\n",
        "    return input_source, output_source\n",
        "\n",
        "def prepare_input_output(sources):\n",
        "    # prepare the input and output chunks\n",
        "    input_srcs = []\n",
        "    punc_targs = []\n",
        "    for chunk in sources:\n",
        "        input_source, punctuation_target = extract_punc(chunk, char2vec.chars, output_char2vec.chars)\n",
        "        input_srcs.append(input_source)\n",
        "        punc_targs.append(punctuation_target)\n",
        "\n",
        "    return input_srcs, punc_targs\n",
        "\n",
        "def _prepare_by_pad(sents, max_len, filler):\n",
        "    padded_seq = []\n",
        "    for sent in sents:\n",
        "        s_l = len(sent)\n",
        "        b_n = math.ceil(s_l / max_len)\n",
        "        s_pad = sent + filler * (b_n * max_len - s_l)\n",
        "        padded_seq.append(s_pad)\n",
        "    return padded_seq\n",
        "\n",
        "def process_char_to_idx(input_, target_):\n",
        "    # Characters to indexes\n",
        "    source_shape = [len(input_), len(input_[0])]\n",
        "    input_ = torch.LongTensor([[[char2vec.get_ind(char)] for char in src] for src in input_])\n",
        "    input_ = input_[..., 0]\n",
        "\n",
        "    # Get the target variables\n",
        "    target_ = Variable(output_char2vec.char_code_batch(target_))\n",
        "    # u, counts = np.unique(target_vec, return_counts=True)\n",
        "\n",
        "    return input_, target_\n",
        "\n",
        "def flatten_(lst):\n",
        "    # Flattening a nested list\n",
        "    # Ref: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists\n",
        "    return [item for sublist in lst for item in sublist]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n6XVAWIze9W"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtyvZJ7NzkGh"
      },
      "source": [
        "class DatasetObject(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.data = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sent = self.data[idx]\n",
        "        sent_len = len(sent)\n",
        "\n",
        "        # input_source, punctuation_target = data.extract_punc(sent, char2vec.chars, output_char2vec.chars)\n",
        "\n",
        "        return sent_len, sent\n",
        "\n",
        "\n",
        "def get_data(dataset, train=True):\n",
        "\n",
        "    # Calculate len dataset\n",
        "    data_len = len(dataset)\n",
        "\n",
        "    # First split to train/test\n",
        "    if train:\n",
        "        sub_dataset = dataset[:int(0.8 * data_len)]\n",
        "    else:\n",
        "        sub_dataset = dataset[int(0.8 * data_len):]\n",
        "\n",
        "    # Create the dataset object\n",
        "    train_or_test = DatasetObject(dataset=sub_dataset)\n",
        "\n",
        "    return train_or_test\n",
        "\n",
        "\n",
        "def create_data_loader(dataset, batch_size):\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True, num_workers=0, drop_last=True)\n",
        "    return loader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJpsqfhnzshk"
      },
      "source": [
        "## Model\n",
        "\n",
        "Here, the model is defined. It's a simple model consists of:\n",
        "\n",
        "\n",
        "\n",
        "1.   An embedding layer defined by `nn.Embedding(input_size, hidden_size)`\n",
        "2.   A bi-directional GRU by `nn.GRU(hidden_size, hidden_size, self.num_layers, bidirectional=bidirectional, batch_first=True)`\n",
        "3. A dense layer as the decoder for classification by `nn.Linear(hidden_size * self.num_directions, output_size)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w_tY0U21O0s"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.num_layers = 2\n",
        "        self.input_size = char2vec.size\n",
        "        self.output_size = output_char2vec.size\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.hidden_size = 32\n",
        "        self.bidirectional = True\n",
        "        self.num_directions = 2 if self.bidirectional else 1\n",
        "        \n",
        "        # Layers\n",
        "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.num_layers, bidirectional=self.bidirectional, batch_first=True)\n",
        "        self.decoder = nn.Linear(self.hidden_size * self.num_directions, self.output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embeded = x\n",
        "        gru_output, hidden = self.gru(embeded, hidden.view(self.num_layers * self.num_directions, self.batch_size,\n",
        "                                                           self.hidden_size))\n",
        "        output = self.decoder(gru_output.contiguous().view(-1, self.hidden_size * self.num_directions))\n",
        "        return output.view(self.batch_size, -1, self.output_size), hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GylWVeT31b5I"
      },
      "source": [
        "## Core\n",
        "\n",
        "The body of running code. It consists of the following functions.\n",
        "\n",
        "- <font color=blue> make() </font>: It creates the data_loaders, model, optimizer, etc.\n",
        "- <font color=blue> train() </font>: Overall training process.\n",
        "- <font color=blue> train_batch() </font>: the operations for each batch to train the model.\n",
        "- <font color=blue> test() </font>: The test pipeline on the whole test dataset.\n",
        "- <font color=blue> load_pretrained_or_not() </font>: Whether to load the pretrained model or used the latest model updated in the training process.\n",
        "- <font color=blue> _run() </font>: Run the whole pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Fs61R_3Gv9"
      },
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "    train, test = get_data(loaded_dataset, train=True), get_data(loaded_dataset, train=False)\n",
        "    train_loader = create_data_loader(train, batch_size=config['batch_size'])\n",
        "    test_loader = create_data_loader(test, batch_size=config['batch_size'])\n",
        "\n",
        "    \"\"\" # Model\n",
        "    Here we initialize the model with its associated parameters.\n",
        "    \"\"\"\n",
        "    # Model initialization\n",
        "    model = Model(config)\n",
        "\n",
        "    \"\"\" # Optimizer & Loss\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Note: To deal with imbalanced scenario, I used a weighted loss function.\n",
        "    # The coefficients are arbitrary and the first one is associated with \"no punctuation\" case.\n",
        "    # Further details:\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "    # https://discuss.pytorch.org/t/passing-the-weights-to-crossentropyloss-correctly/14731/8\n",
        "    weights = torch.tensor([1., 10., 10., 10., 10., 10.])\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, config):\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config['NUM_EPOCHS']\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in range(config['NUM_EPOCHS']):\n",
        "        for _, (sent_lengths, sources) in enumerate(tqdm(train_loader)):\n",
        "\n",
        "            loss = train_batch(sent_lengths, sources, model, optimizer, criterion)\n",
        "            batch_ct += 1\n",
        "\n",
        "        if (epoch+1) % config['EPOCH_TO_SHOW_PREDICTION'] == 0:\n",
        "            print('\\n Epoch {:d} Batch {}'.format(epoch + 1, batch_ct))\n",
        "            print(\"------------------------------\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Get data\n",
        "                test_sent_lengths, test_sources = next(iter(test_loader))\n",
        "\n",
        "                max_len_test = int(max(test_sent_lengths))\n",
        "                # seq_len_test = data.fuzzy_chunk_len(max_len_test, seq_length)\n",
        "\n",
        "                # Prepare and process\n",
        "                input_srcs_test, punc_targs_test = prepare_input_output(test_sources)\n",
        "\n",
        "                # Pad sequences to have equal length\n",
        "                input_source = _prepare_by_pad(input_srcs_test, max_len_test, filler=[\" \"])\n",
        "                target_punctuation = _prepare_by_pad(punc_targs_test, max_len_test, filler=[\"<nopunc>\"])\n",
        "\n",
        "                # Forward pass\n",
        "                input_, target_ = process_char_to_idx(input_source, target_punctuation)\n",
        "\n",
        "                # Get the embedding\n",
        "                embeded = model.embedding(torch.LongTensor(input_))\n",
        "\n",
        "                # Initialize hidden\n",
        "                hidden = model.init_hidden()\n",
        "\n",
        "                # Forward pass to the model\n",
        "                output, hidden = model(embeded, hidden)\n",
        "\n",
        "                # Prediction probabilities\n",
        "                probs = F.softmax(output.view(-1, model.output_size), dim=1\n",
        "                                  ).view(config['batch_size'], -1, model.output_size)\n",
        "\n",
        "                # Use argmax to extract predicted labels\n",
        "                indexes = torch.argmax(probs, axis=2)\n",
        "\n",
        "                # Predict punctuation\n",
        "                predicted_punctuation = output_char2vec.vec2list_batch(indexes)\n",
        "\n",
        "                ############## Evaluation #############\n",
        "\n",
        "                # Flatten vectors. Initial size: batch_size,_ as a nested list.\n",
        "                pred_to_eval = flatten_(predicted_punctuation)\n",
        "                target_to_eval = flatten_(target_punctuation)\n",
        "\n",
        "                # Calculate precision_recall_fscore\n",
        "                labels = list(set(target_to_eval).union(set(pred_to_eval)))\n",
        "                prf = precision_recall_fscore_support(pred_to_eval, target_to_eval, zero_division=0, labels=labels)\n",
        "                index = ['precision', 'recall', 'f_score', 'support']\n",
        "                df = pd.DataFrame(prf, columns=labels, index=index)\n",
        "\n",
        "                # Cut floating points\n",
        "                df = df.applymap(lambda x: float('%.2f' % (x)))\n",
        "                print('Performance: \\n')\n",
        "                print(df)\n",
        "                print('\\n')\n",
        "\n",
        "            # Add punctuation to the source sentence based on the correct punctuation\n",
        "            target_text = add_punctuation(input_source[0], target_punctuation[0])\n",
        "\n",
        "            # Add punctuation to the source sentence based on the predicted punctuation\n",
        "            predicted_text = add_punctuation(input_source[0],\n",
        "                                     predicted_punctuation[0])\n",
        "            cprint('Desired target text: ', 'red', attrs=['bold'])\n",
        "            print('\\t', target_text)\n",
        "            cprint('Input text: ', 'blue', attrs=['bold'])\n",
        "            print('\\t', ''.join(input_source[0]))\n",
        "            cprint('Predicted text: ', 'green', attrs=['bold'])\n",
        "            print('\\t', predicted_text)\n",
        "\n",
        "\n",
        "        #### Save model after each epoch ####\n",
        "        model_path = 'models/' + \"punctuator_\" + str(epoch+1) + '.pt'\n",
        "        torch.save(model, model_path)\n",
        "\n",
        "\n",
        "def train_batch(sent_lengths, sources, model, optimizer, criterion):\n",
        "\n",
        "    # Get the max len of sent\n",
        "    max_len = int(max(sent_lengths))\n",
        "\n",
        "    # Process & prepare\n",
        "    input_srcs, punc_targs = prepare_input_output(sources)\n",
        "\n",
        "    # Initialize hidden\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    input_ = _prepare_by_pad(input_srcs, max_len, filler=[\" \"])\n",
        "    target_ = _prepare_by_pad(punc_targs, max_len, filler=[\"<nopunc>\"])\n",
        "\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Process input target\n",
        "    input_, target_ = process_char_to_idx(input_, target_)\n",
        "\n",
        "    # Get the embedding\n",
        "    embeded = model.embedding(torch.LongTensor(input_))\n",
        "\n",
        "    # Forward pass to the model\n",
        "    output, hidden = model(embeded, hidden)\n",
        "\n",
        "    #### Calculate loss ####\n",
        "    # Flatten the characters along batches and sequences.\n",
        "    loss = criterion(output.view(-1, model.output_size), target_.view(-1))\n",
        "\n",
        "    # Backward pass â¬…\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    cprint('\\n Testing begun: \\n', 'blue', attrs=['bold'])\n",
        "    with torch.no_grad():\n",
        "        f_score_total, total = 0, 0\n",
        "        for batch_test_num, (test_sent_lengths, test_sources) in enumerate(tqdm(test_loader)):\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Get size\n",
        "                total += len(test_sources)\n",
        "\n",
        "                max_len_test = int(max(test_sent_lengths))\n",
        "                # seq_len_test = data.fuzzy_chunk_len(max_len_test, seq_length)\n",
        "\n",
        "                # Prepare and process\n",
        "                input_srcs_test, punc_targs_test = prepare_input_output(test_sources)\n",
        "\n",
        "                # Pad sequences to have equal length\n",
        "                input_source = _prepare_by_pad(input_srcs_test, max_len_test, filler=[\" \"])\n",
        "                target_punctuation = _prepare_by_pad(punc_targs_test, max_len_test, filler=[\"<nopunc>\"])\n",
        "\n",
        "                # Forward pass\n",
        "                input_, target_ = process_char_to_idx(input_source, target_punctuation)\n",
        "\n",
        "                # Get the embedding\n",
        "                embeded = model.embedding(torch.LongTensor(input_))\n",
        "\n",
        "                # Initialize hidden\n",
        "                hidden = model.init_hidden()\n",
        "\n",
        "                # Forward pass to the model\n",
        "                output, hidden = model(embeded, hidden)\n",
        "\n",
        "                # Prediction probabilities\n",
        "                probs = F.softmax(output.view(-1, model.output_size), dim=1\n",
        "                                  ).view(config['batch_size'], -1, model.output_size)\n",
        "\n",
        "                # Use argmax to extract predicted labels\n",
        "                indexes = torch.argmax(probs, axis=2)\n",
        "\n",
        "                # Predict punctuation\n",
        "                predicted_punctuation = output_char2vec.vec2list_batch(indexes)\n",
        "\n",
        "                ############## Evaluation #############\n",
        "\n",
        "                # Flatten vectors. Initial size: batch_size,_ as a nested list.\n",
        "                pred_to_eval = flatten_(predicted_punctuation)\n",
        "                target_to_eval = flatten_(target_punctuation)\n",
        "\n",
        "                # Calculate precision_recall_fscore\n",
        "                labels = list(set(target_to_eval).union(set(pred_to_eval)))\n",
        "                prf = precision_recall_fscore_support(pred_to_eval, target_to_eval, zero_division=0, labels=labels)\n",
        "                index = ['precision', 'recall', 'f_score', 'support']\n",
        "                df = pd.DataFrame(prf, columns=labels, index=index)\n",
        "                f_score_total += df.loc['f_score', :]\n",
        "\n",
        "        print(f\"F1-score: {100 * f_score_total / (batch_test_num + 1)}%\")\n",
        "\n",
        "        # Add punctuation to the source sentence based on the correct punctuation\n",
        "        target_text = add_punctuation(input_source[0], target_punctuation[0])\n",
        "\n",
        "        # Add punctuation to the source sentence based on the predicted punctuation\n",
        "        predicted_text = add_punctuation(input_source[0],\n",
        "                                  predicted_punctuation[0])\n",
        "        cprint('Desired target text: ', 'red', attrs=['bold'])\n",
        "        print('\\t', target_text)\n",
        "        cprint('Input text: ', 'blue', attrs=['bold'])\n",
        "        print('\\t', ''.join(input_source[0]))\n",
        "        cprint('Predicted text: ', 'green', attrs=['bold'])\n",
        "        print('\\t', predicted_text)\n",
        "\n",
        "\n",
        "def load_pretrained_or_not(model, pretrained=False):\n",
        "\n",
        "    if pretrained:\n",
        "        last_saved_epoch = config['PRETRAINED_EPOCH']\n",
        "        model_path = 'models/' + \"punctuator_\" + str(last_saved_epoch) + '.pt'\n",
        "        model = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "        return model\n",
        "    else:\n",
        "        return model\n",
        "\n",
        "\n",
        "def _run(config):\n",
        "\n",
        "    # make the model, data, and optimization problem\n",
        "    model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "\n",
        "    if config['TRAINING']:\n",
        "        # and use them to train the model\n",
        "        train(model, train_loader, test_loader, criterion, optimizer, config)\n",
        "\n",
        "        # Use the model at the end of training\n",
        "        model = load_pretrained_or_not(model, pretrained=False)\n",
        "\n",
        "    else:\n",
        "        # Load pretrained model\n",
        "        model = load_pretrained_or_not(model, pretrained=True)\n",
        "\n",
        "    # # and test its final performance\n",
        "    test(model, test_loader)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESrxLzAW2syF"
      },
      "source": [
        "## Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-CQt4ef2tFo",
        "outputId": "57ba58dd-4f10-4f38-825a-390bf2038812"
      },
      "source": [
        "# Run the training\n",
        "model = _run(config)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/222 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m\u001b[34m\n",
            " Testing begun: \n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 222/222 [01:39<00:00,  2.23it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-score: .           93.101200\n",
            "<cap>       83.845825\n",
            "<nopunc>    99.659339\n",
            "!                 NaN\n",
            "?           71.055812\n",
            ",           51.374920\n",
            "Name: f_score, dtype: float64%\n",
            "\u001b[1m\u001b[31mDesired target text: \u001b[0m\n",
            "\t The one physical system, the brain, contains an accurate working model of the other  the quasar.                                                                                                                                                                                                                                                                                       \n",
            "\u001b[1m\u001b[34mInput text: \u001b[0m\n",
            "\t the one physical system the brain contains an accurate working model of the other  the quasar                                                                                                                                                                                                                                                                                       \n",
            "\u001b[1m\u001b[32mPredicted text: \u001b[0m\n",
            "\t The one physical system, the brain contains, an accurate working model of the other  the Quasar.                                                                                                                                                                                                                                                                                       \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}